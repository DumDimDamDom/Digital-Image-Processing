{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from tensorflow.data import (\n",
    "    AUTOTUNE,\n",
    "    Dataset,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomZoom,\n",
    ")\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from typing import List\n",
    "\n",
    "from src import (\n",
    "    Dataset as WSI_Dataset,\n",
    "    ModelContext,\n",
    "    ModelFactory,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DATA_AUGMENTATION = True\n",
    "\n",
    "MODEL_RECOMPILE = True\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.00001\n",
    "OPTIMIZER = Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS_FUNCTION = SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "FIT_CALLBACKS = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=f\"logs/fit/{MODEL_FILENAME}\",\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts: List[ModelContext] = ModelFactory.models()\n",
    "\n",
    "model_list: List[ModelContext] = list(\n",
    "    filter(\n",
    "        lambda c: c.filename == MODEL_FILENAME,\n",
    "        contexts\n",
    "    )\n",
    ")\n",
    "\n",
    "if len(model_list) < 1:\n",
    "    logger.error(f\"Model {MODEL_FILENAME} not found\")\n",
    "    exit(1)\n",
    "\n",
    "context = model_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_train_ds, _raw_val_ds, _raw_test_ds = WSI_Dataset.get(validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "class_names = _raw_train_ds.class_names\n",
    "\n",
    "logger.info(f\"Raw train set with {len(_raw_train_ds)} samples and {len(_raw_train_ds.class_names)} of classes, which are {', '.join(_raw_train_ds.class_names)}\")\n",
    "logger.info(f\"Raw validation set with {len(_raw_val_ds)} samples and {len(_raw_val_ds.class_names)} of classes, which are {', '.join(_raw_val_ds.class_names)}\")\n",
    "logger.info(f\"Raw test set with {len(_raw_test_ds)} samples and {len(_raw_test_ds.class_names)} of classes, which are {', '.join(_raw_test_ds.class_names)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_ds(ds: Dataset, batch: int, shuffle: bool) -> Dataset:\n",
    "    ds = ds.batch(batch)\n",
    "\n",
    "    if shuffle:\n",
    "        ds.shuffle(buffer_size=500, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.cache()\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def _augment(ds: Dataset) -> Dataset:\n",
    "    data_augmentation = Sequential(\n",
    "        [\n",
    "            RandomFlip(\"horizontal\"),\n",
    "            RandomRotation(0.1),\n",
    "            RandomZoom(0.1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = ds.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if DATA_AUGMENTATION:\n",
    "    _raw_train_ds = _augment(_raw_train_ds)\n",
    "\n",
    "\n",
    "train_ds = _process_ds(_raw_train_ds, batch=BATCH_SIZE, shuffle=True)\n",
    "val_ds = _process_ds(_raw_val_ds, batch=BATCH_SIZE, shuffle=False)\n",
    "test_ds = _process_ds(_raw_test_ds, batch=1, shuffle=False)\n",
    "\n",
    "logger.info(f\"Batched train set with {len(train_ds)} samples\")\n",
    "logger.info(f\"Batched validation set with {len(val_ds)} samples\")\n",
    "logger.info(f\"Test set with {len(test_ds)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_RECOMPILE:\n",
    "    context.model.compile(\n",
    "        optimizer=OPTIMIZER,\n",
    "        loss=LOSS_FUNCTION,\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "context.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit\n",
    "\n",
    "context.model.fit(\n",
    "    train_ds,\n",
    "    callbacks=FIT_CALLBACKS,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = context.model.predict(\n",
    "    test_ds,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "logger.debug(f\"Predictions shape: {predictions.shape}\")\n",
    "logger.debug(f\"Predictions\\n{predictions}\")\n",
    "\n",
    "\n",
    "actual = np.array([l.numpy() for _, l in test_ds])\n",
    "predicted = np.argmax(predictions, axis=-1)\n",
    "\n",
    "logger.debug(f\"Actual shape: {actual.shape}\")\n",
    "logger.debug(f\"Actual values\\n{actual}\")\n",
    "\n",
    "logger.debug(f\"Predicted shape: {predicted.shape}\")\n",
    "logger.debug(f\"Predicted values\\n{predicted}\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "logger.debug(f\"Confusion Matrix\\n{cm}\")\n",
    "\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=class_names,\n",
    ")\n",
    "\n",
    "cm_display.plot(\n",
    "    cmap=\"Blues\",\n",
    "    ax=plt.subplots(figsize=(9, 9))[1]\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "logger.info(f\"Accuracy: {accuracy_score(actual, predicted)}\")\n",
    "logger.info(f\"Precision: {precision_score(actual, predicted, average='micro')}\")\n",
    "logger.info(f\"Sensitivity recall: {recall_score(actual, predicted, average='micro')}\")\n",
    "logger.info(f\"Specificity: {recall_score(actual, predicted, pos_label=0, average='micro')}\")\n",
    "logger.info(f\"F1 score: {f1_score(actual, predicted, average='micro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.model.summary()\n",
    "context.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wood-species-identification-hpklw3A5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

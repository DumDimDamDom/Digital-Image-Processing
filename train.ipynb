{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wood Species Identification Model - Required Modules Explanation\n",
    "\n",
    "This document outlines the modules required for building a wood species identification model, explaining their use cases.\n",
    "\n",
    "## TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow**: An open-source machine learning framework used for both research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud.\n",
    "- **Keras**: A high-level neural networks API, written in Python and capable of running on top of TensorFlow. It enables fast experimentation with deep neural networks.\n",
    "\n",
    "### Key Components from TensorFlow and Keras:\n",
    "\n",
    "- **`Sequential`**: A linear stack of layers.\n",
    "- **`Model`**: The base class for Keras models.\n",
    "- **`Dense`, `Conv2D`, `MaxPool2D`, `Flatten`, `Dropout`**: Layers that are used to build neural networks.\n",
    "- **`RandomFlip`, `RandomRotation`, `RandomZoom`, `Rescaling`**: Data augmentation layers to help the model generalize better.\n",
    "- **`SparseCategoricalCrossentropy`**: A loss function used for classification tasks.\n",
    "- **`SparseCategoricalAccuracy`**: A metric to compute the accuracy rates.\n",
    "- **`Adam`**: An optimizer for training neural networks.\n",
    "- **`EarlyStopping`, `ModelCheckpoint`, `TensorBoard`**: Callbacks for monitoring and improving training.\n",
    "\n",
    "## Keras Tuner\n",
    "\n",
    "- **Keras Tuner**: A library for hyperparameter tuning for Keras models. It helps to pick the optimal set of hyperparameters for your model.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **`Hyperband`**: An optimization algorithm based on the Hyperband algorithm.\n",
    "- **`HyperParameters`**: A class for defining and managing hyperparameters.\n",
    "\n",
    "## TensorFlow Data\n",
    "\n",
    "- **`AUTOTUNE`**: A special value that can be used to indicate that the dataset should tune the number of elements to prefetch dynamically at runtime.\n",
    "- **`Dataset`**: Provides a way to create and manipulate sequences of data items.\n",
    "\n",
    "## Custom Modules\n",
    "\n",
    "- **`src.Dataset` (WSI_Dataset)**: A custom module for loading and preprocessing the wood species identification dataset.\n",
    "- **`ModelContext`, `ModelFactory`**: Custom modules for managing model lifecycle and factory patterns for creating models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras_tuner import (\n",
    "    Hyperband,\n",
    "    HyperParameters,\n",
    ")\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from tensorflow.data import (\n",
    "    AUTOTUNE,\n",
    "    Dataset,\n",
    ")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import (\n",
    "    Model,\n",
    "    regularizers,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPool2D,\n",
    "    MaxPooling2D,\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomZoom,\n",
    "    Rescaling,\n",
    ")\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.models import (\n",
    "    clone_model,\n",
    "    Sequential,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    List,\n",
    "    Optional,\n",
    ")\n",
    "\n",
    "from src import (\n",
    "    Dataset as WSI_Dataset,\n",
    "    ModelContext,\n",
    "    ModelFactory,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wood Species Identification Model - Constants and Callbacks Explanation\n",
    "\n",
    "## Constants\n",
    "\n",
    "### Image Dimensions\n",
    "- `IMG_WIDTH` and `IMG_HEIGHT`: These constants define the width and height of the images that the model will process. Both are set to 200 pixels, ensuring that all images fed into the model are of uniform size.\n",
    "\n",
    "### Model Overwriting\n",
    "- `OVERWRITE_MODEL`: This optional string variable determines whether an existing model should be overwritten. If set to `None`, a new model will be trained.\n",
    "\n",
    "### Data Augmentation\n",
    "- `DATA_AUGMENTATION`: A boolean flag indicating whether data augmentation should be used during training. Data augmentation can help improve model generalization.\n",
    "- `DATA_AUGMENTATION_LAYERS`: A list of data augmentation layers that will be applied if `DATA_AUGMENTATION` is `True`. It includes horizontal flipping, random rotations, and random zooms.\n",
    "\n",
    "### Training Parameters\n",
    "- `BATCH_SIZE`: The number of samples that will be propagated through the network in one forward/backward pass. It is set to 4.\n",
    "- `VALIDATION_SPLIT`: The fraction of the data to be used as validation data. Here, it's set to 30% of the data.\n",
    "- `FINAL_LAYER_UNITS`: The number of neurons in the final layer of the model, which should match the number of classes in the dataset. For this model, it is set to 12.\n",
    "\n",
    "## Callbacks\n",
    "\n",
    "### `HYPERMODEL_CREATION_CALLBACK`\n",
    "This function is crucial for creating a hypermodel with tunable parameters. It takes a `HyperParameters` object and an optional `Model` object as inputs and returns a compiled model. The function allows for tuning various aspects of the model architecture and compilation settings, including:\n",
    "- The number of filters and kernel size for convolutional layers.\n",
    "- L2 regularization strength for convolutional and dense layers.\n",
    "- Dropout rates to prevent overfitting.\n",
    "- The number of units in the dense layer.\n",
    "- The learning rate for the Adam optimizer.\n",
    "\n",
    "This dynamic creation of the model architecture enables efficient hyperparameter tuning to find the best model configuration.\n",
    "\n",
    "### `FIT_CALLBACKS`\n",
    "This function returns a list of callbacks to be used during model training. These callbacks include:\n",
    "- `EarlyStopping`: Monitors the validation loss and stops training if it does not improve for a specified number of epochs (`patience=15`), helping to prevent overfitting.\n",
    "- `ModelCheckpoint`: Saves the model after every epoch where there is an improvement in validation accuracy, ensuring that the best model is retained.\n",
    "- `TensorBoard`: Enables visualization of the training process, including metrics and model architecture, facilitating debugging and optimization.\n",
    "\n",
    "These callbacks are essential for monitoring the training process, saving the best model, and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 200\n",
    "\n",
    "\n",
    "# The name of the model to be trained, should not include the file extension\n",
    "# None if a new model is to be trained\n",
    "OVERWRITE_MODEL: Optional[str] = None\n",
    "\n",
    "\n",
    "# Set to True if data augmentation is to be used\n",
    "DATA_AUGMENTATION: bool = False\n",
    "# The `DATA_AUGMENTATION_LAYERS` will be used only if `DATA_AUGMENTATION` is True\n",
    "DATA_AUGMENTATION_LAYERS = [\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1),\n",
    "]\n",
    "\n",
    "\n",
    "# Variables for the model\n",
    "BATCH_SIZE: int = 4\n",
    "VALIDATION_SPLIT: float = 0.3\n",
    "\n",
    "\n",
    "# The number of neurons in the final layer, should be 12 for this dataset\n",
    "FINAL_LAYER_UNITS: int = 12\n",
    "\n",
    "\n",
    "def HYPERMODEL_CREATION_CALLBACK(\n",
    "    hp: HyperParameters,\n",
    "    *,\n",
    "    model: Optional[Model]=None,\n",
    ") -> Model:\n",
    "    \"\"\"The function to create a hypermodel\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Rescaling(1. / 255),\n",
    "                Conv2D(\n",
    "                    filters=hp.Int(\"conv_1_filter\", min_value=32, max_value=128, step=16),\n",
    "                    kernel_size=hp.Choice(\"conv_1_kernel\", values=[3, 5]),\n",
    "                    kernel_regularizer=regularizers.l2(hp.Choice(\"conv_1_l2\", values=[1e-3, 1e-4, 1e-5, 1e-6])),\n",
    "                    activation=\"relu\",\n",
    "                    input_shape=(None, IMG_WIDTH, IMG_HEIGHT, 3),\n",
    "                ),\n",
    "                MaxPooling2D(\n",
    "                ),\n",
    "                Dropout(\n",
    "                    rate=hp.Float(\"dropout_1_rate\", min_value=0.1, max_value=0.8, step=0.1),\n",
    "                ),\n",
    "                Conv2D(\n",
    "                    filters=hp.Int(\"conv_2_filter\", min_value=32, max_value=128, step=16),\n",
    "                    kernel_regularizer=regularizers.l2(hp.Choice(\"conv_2_l2\", values=[1e-3, 1e-4, 1e-5, 1e-6])),\n",
    "                    kernel_size=hp.Choice(\"conv_2_kernel\", values=[3, 5]),\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                MaxPooling2D(\n",
    "                ),\n",
    "                Dropout(\n",
    "                    rate=hp.Float(\"dropout_2_rate\", min_value=0.1, max_value=0.8, step=0.1),\n",
    "                ),\n",
    "                Flatten(\n",
    "                    input_shape=(None, IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "                ),\n",
    "                Dense(\n",
    "                    units=hp.Int(\"dense_1_units\", min_value=32, max_value=512, step=32),\n",
    "                    kernel_regularizer=regularizers.l2(hp.Choice(\"dense_1_l2\", values=[1e-3, 1e-4, 1e-5, 1e-6])),\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                Dropout(\n",
    "                    rate=hp.Float(\"dropout_3_rate\", min_value=0.1, max_value=0.5, step=0.1),\n",
    "                ),\n",
    "                Dense(\n",
    "                    units=FINAL_LAYER_UNITS,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=SparseCategoricalCrossentropy(\n",
    "            from_logits=True\n",
    "        ),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def FIT_CALLBACKS(model_name: str) -> List[Callable]:\n",
    "    \"\"\"The callbacks to be called after done of each epoch\n",
    "    \"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=15,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"caches/checkpoints/{model_name}.keras\",\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=f\"logs/fit/{model_name}\",\n",
    "            histogram_freq=1,\n",
    "            profile_batch=0,\n",
    "        ),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the Dataset for Wood Species Identification\n",
    "\n",
    "This section demonstrates how to retrieve the dataset split into training, validation, and test sets using a custom dataset class `WSI_Dataset`. This class likely encapsulates the logic for downloading, preprocessing, and partitioning the dataset according to a specified validation split ratio.\n",
    "\n",
    "The dataset is split as follows:\n",
    "- **Training set**: Used to train the model.\n",
    "- **Validation set**: Used to tune the hyperparameters and evaluate the model during training.\n",
    "- **Test set**: Used to test the model's performance after training is complete.\n",
    "\n",
    "The `class_names` attribute of the dataset object contains the names of the wood species, which are logged along with the number of samples in each dataset split. This information is crucial for understanding the composition of the dataset and ensuring that the model is trained on a balanced and diverse set of images.\n",
    "\n",
    "For more details on the dataset and to access it, visit the dataset's page on Hugging Face: [Wood Species Identification Dataset](https://huggingface.co/datasets/LynBean/wood-species-identification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "_raw_train_ds, _raw_val_ds, _raw_test_ds = WSI_Dataset.get(validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "class_names = _raw_train_ds.class_names\n",
    "\n",
    "logger.info(f\"Raw train set with {len(_raw_train_ds)} samples and {len(_raw_train_ds.class_names)} of classes, which are {', '.join(_raw_train_ds.class_names)}\")\n",
    "logger.info(f\"Raw validation set with {len(_raw_val_ds)} samples and {len(_raw_val_ds.class_names)} of classes, which are {', '.join(_raw_val_ds.class_names)}\")\n",
    "logger.info(f\"Raw test set with {len(_raw_test_ds)} samples and {len(_raw_test_ds.class_names)} of classes, which are {', '.join(_raw_test_ds.class_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Explanation\n",
    "\n",
    "The provided code snippet from `train.ipynb` outlines the data preprocessing steps for a machine learning model focused on wood species identification. The preprocessing involves two main functions: `_process_ds` and `_augment`, along with conditional data augmentation based on the `DATA_AUGMENTATION` flag.\n",
    "\n",
    "## `_process_ds` Function\n",
    "This function takes a TensorFlow `Dataset` object, a batch size, and a shuffle flag as inputs. It performs the following operations:\n",
    "1. **Batching**: Groups the dataset into batches of the specified size using `ds.batch(batch)`.\n",
    "2. **Shuffling**: If `shuffle` is `True`, the dataset is shuffled with a buffer size of 500 to ensure randomness. This is particularly useful for the training dataset to prevent the model from learning the order of the data.\n",
    "3. **Caching**: The dataset is cached using `ds.cache()` to improve performance by storing the dataset in memory after the first epoch, reducing read latency in subsequent epochs.\n",
    "4. **Prefetching**: `ds.prefetch(buffer_size=AUTOTUNE)` allows the dataset to prefetch batches while the model is training, improving efficiency by reducing the time the model spends waiting for data.\n",
    "\n",
    "## `_augment` Function\n",
    "This function applies data augmentation to the dataset if the `DATA_AUGMENTATION` flag is set to `True`. It uses a `Sequential` model of `DATA_AUGMENTATION_LAYERS` to apply transformations such as flipping, rotation, and zooming on the images. Data augmentation is performed on-the-fly during training, effectively increasing the diversity of the training data without requiring additional storage.\n",
    "\n",
    "## Conditional Data Augmentation\n",
    "The dataset is conditionally augmented based on the `DATA_AUGMENTATION` flag. If `True`, both the raw training and validation datasets are augmented using the `_augment` function. This step enhances the model's ability to generalize by training on a more varied dataset.\n",
    "\n",
    "## Final Dataset Preparation\n",
    "Finally, the training, validation, and test datasets are processed using the `_process_ds` function with appropriate batching and shuffling settings. The training dataset is shuffled to ensure randomness, while the validation and test datasets do not need shuffling. The test dataset is batched with a size of 1, as each sample is evaluated independently.\n",
    "\n",
    "The code concludes with logging the number of samples in the batched train, validation, and test datasets, providing insight into the dataset size and composition after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "def _process_ds(ds: Dataset, batch: int, shuffle: bool) -> Dataset:\n",
    "    ds = ds.batch(batch)\n",
    "\n",
    "    if shuffle:\n",
    "        ds.shuffle(buffer_size=500, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.cache()\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def _augment(ds: Dataset) -> Dataset:\n",
    "    data_augmentation = Sequential(DATA_AUGMENTATION_LAYERS)\n",
    "\n",
    "    result = ds.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y),\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "if DATA_AUGMENTATION:\n",
    "    _raw_train_ds = _augment(_raw_train_ds)\n",
    "    _raw_val_ds = _augment(_raw_val_ds)\n",
    "    logger.info(\"Dataset has been augmented\")\n",
    "\n",
    "\n",
    "train_ds = _process_ds(_raw_train_ds, batch=BATCH_SIZE, shuffle=True)\n",
    "val_ds = _process_ds(_raw_val_ds, batch=BATCH_SIZE, shuffle=False)\n",
    "test_ds = _process_ds(_raw_test_ds, batch=1, shuffle=False)\n",
    "\n",
    "logger.info(f\"Batched train set with {len(train_ds)} samples\")\n",
    "logger.info(f\"Batched validation set with {len(val_ds)} samples\")\n",
    "logger.info(f\"Test set with {len(test_ds)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of First Image from Each Class\n",
    "\n",
    "This section is designed to visualize the first image from each class label in the dataset. It iterates through the training dataset, collecting one unique image per class until it has an image for each class name. These images are then plotted in a grid, with each image labeled with its corresponding class name. This visualization helps in understanding the diversity and characteristics of the dataset's classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "_taken_classes = set()\n",
    "\n",
    "_images_list = []\n",
    "_labels_list = []\n",
    "\n",
    "\n",
    "for images, labels in train_ds:\n",
    "    for i in range(images.shape[0]):\n",
    "\n",
    "        if labels[i].numpy() in _taken_classes:\n",
    "            continue\n",
    "\n",
    "        _taken_classes.add(labels[i].numpy())\n",
    "\n",
    "        _images_list.append(images[i])\n",
    "        _labels_list.append(labels[i])\n",
    "\n",
    "        if len(_taken_classes) >= len(class_names):\n",
    "            break\n",
    "\n",
    "    if len(_taken_classes) >= len(class_names):\n",
    "        break\n",
    "\n",
    "\n",
    "_images_tensor = tf.stack(_images_list)\n",
    "_labels_tensor = tf.stack(_labels_list)\n",
    "\n",
    "logger.debug(f\"Images tensor shape: {_images_tensor.shape}\")\n",
    "logger.debug(f\"Labels tensor shape: {_labels_tensor.shape}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "for image, label in zip(_images_tensor, _labels_tensor):\n",
    "    plt.subplot(3, 4, label.numpy() + 1)\n",
    "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if OVERWRITE_MODEL is not None:\n",
    "    _contexts: List[ModelContext] = ModelContext.models()\n",
    "    context: Optional[ModelContext] = next(\n",
    "        filter(lambda x: x.name == OVERWRITE_MODEL, _contexts),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if context is None:\n",
    "        raise ValueError(f\"Model {OVERWRITE_MODEL} not found\")\n",
    "\n",
    "    logger.info(f\"Model {context.name} will be used for this training\")\n",
    "\n",
    "\n",
    "else:\n",
    "    context = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Hyperband\n",
    "\n",
    "This section demonstrates the process of hyperparameter tuning for a wood species identification model using the Hyperband algorithm. Hyperparameter tuning is crucial for optimizing the model's performance by finding the best set of parameters.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Hyperband Initialization**: The `Hyperband` tuner is initialized with a custom model creation function (`HYPERMODEL_CREATION_CALLBACK`), which dynamically constructs a model based on the current set of hyperparameters (`hp`) being evaluated. The tuner aims to maximize validation accuracy (`objective=\"val_accuracy\"`) over a maximum of 200 epochs, adjusting the number of models trained in each iteration by a factor of 3.\n",
    "\n",
    "2. **Tuner Search**: The `tuner.search` method starts the hyperparameter search process. It trains different configurations of the model on the `train_ds` dataset, evaluates them on `val_ds`, and uses the callbacks returned by `FIT_CALLBACKS(\"hyperband\")` for early stopping and model checkpointing.\n",
    "\n",
    "3. **Best Hyperparameters**: After the search completes, the best set of hyperparameters is retrieved using `tuner.get_best_hyperparameters(num_trials=1)[0]`. These parameters are logged for debugging and success reporting, showcasing the optimal values for each hyperparameter.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **Hyperband Algorithm**: An efficient and effective hyperparameter optimization algorithm that uses a \"successive halving\" approach. It dynamically allocates resources to more promising configurations.\n",
    "\n",
    "- **`HYPERMODEL_CREATION_CALLBACK`**: A function that takes hyperparameters as input and returns a compiled model. This allows for flexible model architecture adjustments based on the hyperparameters being tested.\n",
    "\n",
    "- **Callbacks**: Used during the search to implement strategies like early stopping (to prevent overfitting) and model checkpointing (to save the best model).\n",
    "\n",
    "This process is essential for fine-tuning the model to achieve the best possible accuracy on the validation dataset, leading to improved performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "%tensorboard --logdir logs/fit\n",
    "\n",
    "\n",
    "initial_model: Optional[Model] = clone_model(context.model) if context is not None else None\n",
    "\n",
    "\n",
    "tuner = Hyperband(\n",
    "    lambda hp: HYPERMODEL_CREATION_CALLBACK(\n",
    "        hp,\n",
    "        model=initial_model,\n",
    "    ),\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=200,\n",
    "    factor=3,\n",
    "    directory=\"caches\",\n",
    "    project_name=\"hyperband\",\n",
    ")\n",
    "\n",
    "\n",
    "tuner.search(\n",
    "    train_ds,\n",
    "    epochs=200,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=FIT_CALLBACKS(\"hyperband\"),\n",
    ")\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "logger.debug(f\"Best HPs: {best_hps}\")\n",
    "logger.success(\n",
    "    f\"The hyperparameter search is complete. The optimal values are\\n\" + \\\n",
    "    \"\\n\".join([f\"{k.capitalize():25s}: {v}\" for k, v in best_hps.values.items()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Hypermodel with Optimal Hyperparameters\n",
    "\n",
    "After completing the hyperparameter tuning process, the next step involves building and training the model using the best hyperparameters found. This section outlines this process, ensuring that the model is trained efficiently to achieve the best performance on the validation dataset.\n",
    "\n",
    "## Building the Hypermodel\n",
    "\n",
    "**Model Construction**: The model is constructed using the `tuner.hypermodel.build(best_hps)` method, where `best_hps` contains the optimal set of hyperparameters discovered during the tuning process. This step ensures that the model architecture is configured with the best parameters for training.\n",
    "\n",
    "## Initial Training\n",
    "\n",
    "1. **Model Training**: The model is trained using the `context.model.fit` method with the training dataset (`train_ds`), validation dataset (`val_ds`), and a set of callbacks returned by `FIT_CALLBACKS(context.name)`. The training is set to run for a large number of epochs (`10000`), but early stopping is expected to halt training when no improvement is observed.\n",
    "\n",
    "2. **Evaluation**: After training, the model is evaluated on the test dataset (`test_ds`) to obtain the test loss and accuracy, which are logged for analysis.\n",
    "\n",
    "## Identifying the Best Epoch\n",
    "\n",
    "1. **Best Epoch Calculation**: The epoch that achieved the highest validation accuracy during training is identified. This is done by finding the maximum value in `history.history[\"val_accuracy\"]` and adding one (since epochs are zero-indexed).\n",
    "\n",
    "2. **Logging the Best Epoch**: A log message is generated to indicate the best epoch for training.\n",
    "\n",
    "## Final Training\n",
    "\n",
    "1. **Rebuilding the Model**: The model is rebuilt with the optimal hyperparameters to reset its state.\n",
    "\n",
    "2. **Final Training Run**: The model undergoes a final training run with the previously identified best epoch number. This approach ensures that the model is not overfitted to the validation dataset.\n",
    "\n",
    "This process of building the hypermodel with the best hyperparameters, identifying the best epoch, and conducting a final training run optimizes the model's performance, ensuring it is well-tuned for making accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "__model: Model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "\n",
    "if context is not None:\n",
    "    context.model = __model\n",
    "    logger.info(f\"Re-using the model {context.name}\")\n",
    "\n",
    "else:\n",
    "    context = ModelFactory.create(__model)\n",
    "    logger.info(f\"Created a new model {context.name}\")\n",
    "\n",
    "\n",
    "context.model.summary(\n",
    "    expand_nested=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = context.model.fit(\n",
    "    train_ds,\n",
    "    callbacks=FIT_CALLBACKS(context.name),\n",
    "    validation_data=val_ds,\n",
    "    epochs=10000,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "eval_result = context.model.evaluate(test_ds)\n",
    "logger.info(f\"Test loss: {eval_result[0]}\")\n",
    "logger.info(f\"Test accuracy: {eval_result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_epoch = history.history[\"val_accuracy\"].index(\n",
    "    max(history.history[\"val_accuracy\"])\n",
    ") + 1\n",
    "\n",
    "\n",
    "logger.debug(f\"Best epoch: {best_epoch}\")\n",
    "logger.info(f\"Re-instantiate the hypermodel and train it with the optimal number of epochs {best_epoch}.\")\n",
    "\n",
    "context.model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "context.model.fit(\n",
    "    train_ds,\n",
    "    callbacks=FIT_CALLBACKS(context.name),\n",
    "    validation_data=val_ds,\n",
    "    epochs=best_epoch,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "eval_result = context.model.evaluate(test_ds)\n",
    "logger.info(f\"Test loss: {eval_result[0]}\")\n",
    "logger.info(f\"Test accuracy: {eval_result[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction and Evaluation\n",
    "\n",
    "This section demonstrates the process of making predictions with the trained model on the test dataset, followed by evaluating the model's performance using various metrics. Here's a breakdown of the steps involved:\n",
    "\n",
    "## Making Predictions\n",
    "\n",
    "- **Model Prediction**: `context.model.predict(test_ds, verbose=1)` generates predictions for the test dataset.\n",
    "\n",
    "## Preparing Actual and Predicted Labels\n",
    "\n",
    "- **Extracting Actual Labels**: Actual labels are extracted from the test dataset and converted into a NumPy array for comparison with the predicted labels.\n",
    "- **Determining Predicted Labels**: The `argmax` function is applied to the predictions to convert the model's output probabilities into class labels.\n",
    "\n",
    "## Model Performance Metrics\n",
    "\n",
    "- **Accuracy**: The proportion of correctly predicted observations to the total observations.\n",
    "- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. Here, it's calculated with `average='micro'` to aggregate the contributions of all classes.\n",
    "- **Sensitivity (Recall)**: The ratio of correctly predicted positive observations to all actual positives. Also calculated with `average='micro'`.\n",
    "- **Specificity**: Calculated as the recall for the negative class, but here it's equivalent to sensitivity since `pos_label=0` and `average='micro'` are used.\n",
    "- **F1 Score**: The weighted average of Precision and Recall. Using `average='micro'` means calculating metrics globally by counting the total true positives, false negatives, and false positives.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "- **Generation**: A confusion matrix is generated using `confusion_matrix(actual, predicted)`, providing a summary of the prediction results.\n",
    "- **Visualization**: The confusion matrix is visualized using `ConfusionMatrixDisplay`, with class names as labels. The plot is displayed with a blue color map to make it easier to interpret the model's performance across different classes.\n",
    "\n",
    "This section of the code is crucial for understanding how well the model performs on unseen data, identifying areas where the model excels or struggles, and guiding further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = context.model.predict(\n",
    "    test_ds,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "logger.debug(f\"Predictions shape: {predictions.shape}\")\n",
    "logger.debug(f\"Predictions\\n{predictions}\")\n",
    "\n",
    "\n",
    "actual = np.array([l.numpy() for _, l in test_ds])\n",
    "predicted = np.argmax(predictions, axis=-1)\n",
    "\n",
    "logger.debug(f\"Actual shape: {actual.shape}\")\n",
    "logger.debug(f\"Actual values\\n{actual}\")\n",
    "\n",
    "logger.debug(f\"Predicted shape: {predicted.shape}\")\n",
    "logger.debug(f\"Predicted values\\n{predicted}\")\n",
    "\n",
    "\n",
    "logger.info(f\"Accuracy: {accuracy_score(actual, predicted)}\")\n",
    "logger.info(f\"Precision: {precision_score(actual, predicted, average='micro')}\")\n",
    "logger.info(f\"Sensitivity recall: {recall_score(actual, predicted, average='micro')}\")\n",
    "logger.info(f\"Specificity: {recall_score(actual, predicted, pos_label=0, average='micro')}\")\n",
    "logger.info(f\"F1 score: {f1_score(actual, predicted, average='micro')}\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "logger.debug(f\"Confusion Matrix\\n{cm}\")\n",
    "\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=class_names,\n",
    ")\n",
    "\n",
    "cm_display.plot(\n",
    "    cmap=\"Blues\",\n",
    "    ax=plt.subplots(figsize=(9, 9))[1]\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary and Saving\n",
    "\n",
    "This section performs two key actions related to the final model:\n",
    "\n",
    "1. **Model Summary**: `context.model.summary()` displays a summary of the model's architecture, including details about the layers, their shapes, and the total number of parameters. This overview is crucial for understanding the model's structure and complexity.\n",
    "\n",
    "2. **Saving the Model**: `context.save()` saves the entire model into a single file. This includes the architecture, weights, and training configuration, allowing for easy deployment or further training in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "context.model.summary()\n",
    "context.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wood-species-identification-hpklw3A5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
